{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  My report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                My Wrangling Report\n",
    "\n",
    "####   I was given three files that contained information about dogs and how they were rated by users in the We Rate Dogs contest. I had to download the first file manually, the second file was downloaded with the Requests library using the Url link that was provided. The third file was scraped from Twitter's API using tweepy. \n",
    "####    Once the files were ready, I read them into a dataframe and visually assessed them using both python and excel. I did this to check if the data was clean, that is the quality and tidiness of the data. Since visually assessing data is not enough, I went ahead and programmatically accessed data. After assessing the data, I found it to be untidy as data types were wrong, there were missing data, wrong data,multiple columns that were meant to be a single column.\n",
    "####    Before I started cleaning I made a copy of all the data frames so I do not affect the original data and also in case someone else wants to reproduce this analysis, they can have access to the original data. I cleaned the name columns by replacing None to NaN, the acceptable input for missing values. I also found some likely names and replaced the wrong names with them. I changed the columns that had data type as integer to string as they were not for calculation but identification. There were unnecessary columns that were not useful for the analysis leading to too much information.I went ahead and dropped them to avoid a cumbersome table. Then I found out that there were outliers(extremely high values), I wanted to drop them, but I decided to replace them with the mean of the values, that way I would not lose important data. For the analysis, I do not need retweets as it is meant for only original tweets. I filtered out rows that had retweet id, that way I was left with only original tweets. Also in the table that contained predictions, there were predictions for other pictures apart from dogs. I filtered out all rows where the predictions were not for dogs.\n",
    "####    Then I had to join the three tables into one as they were all a piece of the 'big picture'. I went ahead and saved the cleaned new dataframe. I started my analysis, I looked for the most popular dog names, the most tweeted dog stages and as it was a competition, I also looked out for the dog that had the highest like.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
